{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Astro 528, Lab 7, Exercise 2\n",
    "## Parallelization for Cluster using Distributed-Memory Model\n",
    "\n",
    "In this lab exercise, we'll perform the calculations very similar to those of exercise 2 of lab 5.  However, instead of using a multi-core workstation, we will [run the calculations on the ICDS Roar Collab cluster](https://www.icds.psu.edu/roar-collab-user-guide/) using a distributed memory model.  \n",
    "\n",
    "You're welcome to run this notebook one cell at a time as in other labs to see how it works.  However, the main point of this lab is to see how to run such a calculation in parallel over multiple processor cores that are not necessarily on the same processor.  Therefore, you'll use the command line interface to submit the jobs [ex2.pbs](https://github.com/PsuAstro528/lab7-start/blob/main/ex2.slurm).  Follow the instructions in the [lab's README](https://github.com/PsuAstro528/lab7-start/blob/main/README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T02:17:42.438000-05:00",
     "start_time": "2019-02-25T07:17:40.141Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `/storage/work/ebf11/Teach/Astro528/Fall2023/lab7-start`\n"
     ]
    }
   ],
   "source": [
    "# Just in case you're running this from JupyterLab\n",
    "import Pkg\n",
    "Pkg.UPDATED_REGISTRY_THIS_SESSION[] = true   \n",
    "Pkg.activate(\".\")\n",
    "# Shouldn't need to instantiate the project, since you'll have already run as part of exercise 1.  But I'll default to running it just in case.\n",
    "Pkg.instantiate()\n",
    "# Again, just being extra careful to make sure everything is precompiled by delegator process before assigning work to worker proceses\n",
    "Pkg.precompile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, packages are installed to disk and the Roar Collab nodes all see the same file systems, so you only want to run `instantiate` and `precompile` once by the delegator process, so multiple workers don't try to compile the same package at the same time.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting setup for parallel computation\n",
    "First, we'll load Julia's `Distributed` module that provides much of the needed functionality for multi-processing.  If julia doesn't already have multiple worker threads, then we'll try to start some. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Julia is using 1 workers.\n"
     ]
    }
   ],
   "source": [
    "using Distributed, CpuId\n",
    "println(\"# Julia is using \", nworkers(), \" workers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[33m\u001b[1mWarning: \u001b[22m\u001b[39mSince there's only one worker, I'm guessing you're running this from inside JupyterLab rather than a dedicated slurm job.\n",
      "\u001b[33m\u001b[1m└ \u001b[22m\u001b[39m\u001b[90m@ Main In[3]:3\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4-element Vector{Int64}:\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if nworkers() == 1\n",
    "    if haskey(ENV,\"SLURM_CPUS_PER_TASK\")\n",
    "        @warn \"Since there's only one worker, I'm guessing you're running this from inside JupyterLab rather than a dedicated slurm job.\"\n",
    "        cores_for_job = parse(Int,ENV[\"SLURM_CPUS_PER_TASK\"])\n",
    "        addprocs(cores_for_job)\n",
    "    elseif haskey(ENV,\"GITHUB_ACTION\")   \n",
    "        @warn \"Not adding any worker processes since running on GitHub.\"\n",
    "    else \n",
    "        @warn \"It appears this is not running on Roar Collab.  So we'll add one worker process for each physical core.\"\n",
    "        addprocs(cpucores())\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Now Julia is using 4 workers.\n"
     ]
    }
   ],
   "source": [
    "println(\"# Now Julia is using \", nworkers(), \" workers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Packages and modules\n",
    "Now, we can start loading other packages that we'll be using for our actual calculations.  \n",
    "First, we'll load the CSV and DataFrames packages that only need to be loaded by the delegator process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T02:17:49.984000-05:00",
     "start_time": "2019-02-25T07:17:40.143Z"
    }
   },
   "outputs": [],
   "source": [
    "using CSV, DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For other packages (here Distributions, DistributedArrays and ParallelDataTransfer), we want the packages to be in scope on each worker.  Therefore, need to add the `@everywhere` macro in front of the `using` statement.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T02:18:01.598000-05:00",
     "start_time": "2019-02-25T07:17:40.150Z"
    }
   },
   "outputs": [],
   "source": [
    "@everywhere using Distributions\n",
    "@everywhere using DistributedArrays\n",
    "@everywhere using ParallelDataTransfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, I've written several functions that will be used to generate simulated spectra.  This serves a couple of purposes.\n",
    "First, you'll use the code in the exercise, so you have a calculation that's big enough to be worth parallelizing.  For the purposes of this exercise, it's not essential that you review the code I provided in `.jl` files.  However, the second purpose of providing this is to demonstrate several of the programming patterns that we've discussed in class.  For example, the code in the `lab7` module\n",
    "- is in the form of several small functions, each which does one specific task.  \n",
    "- has been moved out of the Jupyter notebook and into `.jl` files in the `src` directory.\n",
    "- creates custom types for objectstaht represent spectra and a convolution kernel.\n",
    "- uses [abstract types](https://docs.julialang.org/en/v1/manual/types/#Abstract-Types-1), [parametric types](https://docs.julialang.org/en/v1/manual/types/#Parametric-Types-1), and type-stable functions. \n",
    "- has been  put into a Julia [module](https://docs.julialang.org/en/v1/manual/modules/index.html), so that it can be easily loaded and so as to limit potential for namespace conflicts.\n",
    "\n",
    "You don't need to read all of this code right now.  But, when you're writing code for your class project, you're likely to want to make use of some of these same programming patterns. So, it may be useful to refer back to this code later to help see examples of how to apply these design patterns in practice.  \n",
    "        \n",
    "For now, let's include just the file that has the code for the `lab7` module.  `src/lab7.jl` includes the code from the other files.  We'll preface it with `@everywhere`, since we want all of the processors to be able to make use of these function and types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T02:18:03.022000-05:00",
     "start_time": "2019-02-25T07:17:40.153Z"
    }
   },
   "outputs": [],
   "source": [
    "@everywhere include(\"src/lab7.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll bring that module into scope on each worker process.  Note that since this is not a package, we need to include a `.` to tell Julia that it should load a module in the current namespace, rather than needing to load a package from among those in the general Julia registry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T02:18:03.023000-05:00",
     "start_time": "2019-02-25T07:17:40.155Z"
    }
   },
   "outputs": [],
   "source": [
    "@everywhere using .lab7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize data to be analyzed\n",
    "In this exercise, we're going to create a model spectrum consisting of continuum, stellar absorption lines, telluric absorption lines.  \n",
    "The `lab7` module provides a `SimulatedSpectrum` type, but we'll need to initialize a variable with some specific parameter values.  The function does that for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T02:18:03.506000-05:00",
     "start_time": "2019-02-25T07:17:40.157Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "make_spectrum_object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"Create an object that provides a model for the raw spetrum (i.e., before entering the telescope)\"\n",
    "function make_spectrum_object(;lambda_min = 4500, lambda_max = 7500, flux_scale = 1.0,\n",
    "        num_star_lines = 200, num_telluric_lines = 100, limit_line_effect = 10.0)\n",
    "\n",
    "    continuum_param = flux_scale .* [1.0, 1e-5, -2e-8]\n",
    "    \n",
    "    star_line_locs = rand(Uniform(lambda_min,lambda_max),num_star_lines)\n",
    "    star_line_widths = fill(1.0,num_star_lines)\n",
    "    star_line_depths = rand(Uniform(0,1.0),num_star_lines)\n",
    "    \n",
    "    telluric_line_locs = rand(Uniform(lambda_min,lambda_max),num_telluric_lines)\n",
    "    telluric_line_widths = fill(0.2,num_telluric_lines)\n",
    "    telluric_line_depths = rand(Uniform(0,0.4),num_telluric_lines)\n",
    "\n",
    "    SimulatedSpectrum(star_line_locs,star_line_widths,star_line_depths,telluric_line_locs,telluric_line_widths,telluric_line_depths,continuum_param=continuum_param,lambda_mid=0.5*(lambda_min+lambda_max),limit_line_effect=limit_line_effect)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we: \n",
    "1. create a set of wavelengths to observe the spectrum at, \n",
    "2. call the function above to create a spectrum object, \n",
    "3. create an object containing a model for the point spread function, and \n",
    "4. create an object that can compute the convolution of our spectral model with the point spread function model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T02:18:05.032000-05:00",
     "start_time": "2019-02-25T07:17:40.159Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvolvedSpectrum{SimulatedSpectrum{Float64, Float64, Float64, Float64, Float64, Float64, Float64, Float64, Float64}, GaussianMixtureConvolutionKernel{Float64, Float64}}(SimulatedSpectrum{Float64, Float64, Float64, Float64, Float64, Float64, Float64, Float64, Float64}([5231.820220373545, 6649.693647500714, 7209.009730810318, 5389.085520075995, 4714.758160197621, 6410.149441857935, 6517.527546210812, 6286.524922860792, 7359.346455177612, 5385.521170050503  …  7377.512119043639, 5385.311261012031, 6439.9078709047535, 5057.505738391177, 7232.481452110577, 5612.785805293267, 5056.674768824698, 6638.939559154614, 4731.611950599289, 5342.009365691589], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0  …  1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.705286383823691, 0.7341068790167641, 0.8791323665357921, 0.09328042945058945, 0.00831669524178702, 0.4852043144216922, 0.5236911635506255, 0.7134900835951296, 0.20703381466640414, 0.7880117624579207  …  0.8058108910391614, 0.13945121039136388, 0.2870476462263488, 0.4975284290936761, 0.6647086408103486, 0.1554925954417733, 0.31723930278836476, 0.9954145435796975, 0.20436193427364868, 0.16969908245641996], [5560.38689383876, 7107.673004907184, 6582.504181125458, 6659.15504715458, 5826.180938424008, 7372.845444613526, 4671.76149106324, 6914.596004614713, 5599.0796272054, 6948.628355676035  …  6164.66333073327, 6259.997098367373, 6457.51062640724, 5687.624909101234, 6605.208712683048, 5778.26250290128, 5183.557903836984, 4893.842061365773, 5356.721452048483, 5674.492659195836], [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2  …  0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2], [0.08223772769130121, 0.10969221151553166, 0.24013638574339888, 0.2221293859250667, 0.304630833446909, 0.028734890698303508, 0.21218626054183498, 0.32065604319417307, 0.11037478616798087, 0.20083832576301766  …  0.26309894621962204, 0.07760555339173886, 0.05512967960295301, 0.07710870537519932, 0.1635054698028823, 0.13952880195352796, 0.06356415095104372, 0.06881561084741965, 0.3636037184105142, 0.2198717297509421], [1.0, 1.0e-5, -2.0e-8], 0.0, 6000.0, 10.0), GaussianMixtureConvolutionKernel{Float64, Float64}(Normal{Float64}[Normal{Float64}(μ=0.0, σ=0.5), Normal{Float64}(μ=0.0, σ=1.0), Normal{Float64}(μ=0.0, σ=2.0)], [0.8, 0.15, 0.05], 1.0000000143325827, 10.0), 10.0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.  Pick range of of wavelength to work on.\n",
    "lambda_min = 4500.0\n",
    "lambda_max = 7500.0\n",
    "# You may want to adjust the num_lambda to make things more/less computationally intensive\n",
    "num_lambda = 128*1024\n",
    "lambdas = collect(range(lambda_min,stop=lambda_max, length=num_lambda));\n",
    "\n",
    "# 2.  Create a model  spectrum that we'll analyze below\n",
    "raw_spectrum = make_spectrum_object(lambda_min=lambda_min,lambda_max=lambda_max)\n",
    "\n",
    "# 3.  Create a model for the point spread function (PSF)\n",
    "psf_widths  = [0.5, 1.0, 2.0]\n",
    "psf_weights = [0.8, 0.15, 0.05]\n",
    "psf_model = GaussianMixtureConvolutionKernel(psf_widths,psf_weights)\n",
    "\n",
    "# 4. Create a model for the the convolution of thte raw spectrum with the PDF model\n",
    "conv_spectrum = ConvolvedSpectrum(raw_spectrum,psf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking serial code\n",
    "\n",
    "*If the current job has just one worker process,* then we'll benchmark the calculation of this spectrum on a single processor.  Based on performance results from lab 5, we'll make use of Julia's [dot syntax](https://docs.julialang.org/en/v1/manual/functions/#man-vectorized-1) to [\"broadcast\" and \"fuse\"](https://docs.julialang.org/en/v1/base/arrays/#Broadcast-and-vectorization-1) the array operation.    We'll run it just a few times (rather than using `@btime`).  Since the rest of the notebook is meant for distributed computing, it doesn't make sense to keep running.  We'll tell it to exit here if it's just running the automated tests on GitHub.  Before we do, we'll flush the buffer, so any messages are written to `STDOUT` before the kernel exits.|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T02:18:05.217000-05:00",
     "start_time": "2019-02-25T07:17:40.160Z"
    }
   },
   "outputs": [],
   "source": [
    "num_runs = 2\n",
    "if nworkers() == 1    \n",
    "    @warn \"Since there's only one worker process, the distributed computing examples below won't actually run in parallel.\"\n",
    "    for i in 1:num_runs @time conv_spectrum.(lambdas); end\n",
    "    flush(stdout) # flush buffer before the kernel exits\n",
    "    sleep(1)      # wait for one second just to make sure\n",
    "    if haskey(ENV,\"GITHUB_ACTION\")                                               \n",
    "        exit(0)      # Don't try to do parallel work in GitHub actions\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed Arrays\n",
    "Here, we want to spread the work over processor cores than are not necessarily on the same node, so we need to use a [distributed memory system](https://en.wikipedia.org/wiki/Distributed_memory).  This would be necessary if you wanted your job to run on more cores than are available on a single node.  It could also be useful if you wanted to increase the chances that the scheduler starts your job more quickly, since asking for several processors cores that don't need to be on the same node is easier to accommodate than asking for the same number of cores on a single node.  Here we'll use Julia's [DistributedArrays.jl](https://juliaparallel.github.io/DistributedArrays.jl/latest/index.html) package to make programming for distributed memory systems relatively easy.  \n",
    "\n",
    "Here we'll create a distributed array by simply applying `distribute` to our existing array of wavelengths.  (Remember that we could initialize a `DArray` more efficiently be letting each workers initializes its own data.  For convenience functions like `dzeros`, `dones`, `drand`, `drandn` and `dfill` act similarly to their counterparts without a `d` prefix, but create DArrays instead of regular Arrays.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T02:18:07.340000-05:00",
     "start_time": "2019-02-25T07:17:40.163Z"
    }
   },
   "outputs": [],
   "source": [
    "lambdas_dist = distribute(lambdas);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, the first time we call a function, it takes some extra time and memory to compile it.  So let's do that again, this time benchmarking the `distribute` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T02:18:07.393000-05:00",
     "start_time": "2019-02-25T07:17:40.165Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Timing calls to distribute.\n",
      "  0.001734 seconds (517 allocations: 1.024 MiB)\n"
     ]
    }
   ],
   "source": [
    "println(\"# Timing calls to distribute.\")\n",
    "@time lambdas_dist = distribute(lambdas);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will apply `map` to a `lambda_dist`, which is a `DArray`.  `map` will parallelize the calculation and return the results in a `DArray`.  Each worker operates on the subset of the array that is local to that worker process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T02:19:09.082000-05:00",
     "start_time": "2019-02-25T07:17:40.167Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Timing calls to map(...).\n",
      "  6.728920 seconds (613.16 k allocations: 41.122 MiB, 0.23% gc time, 6.66% compilation time)\n",
      "  4.174013 seconds (913 allocations: 81.266 KiB)\n"
     ]
    }
   ],
   "source": [
    "println(\"# Timing calls to map(...).\")\n",
    "for i in 1:num_runs \n",
    "    @time map(conv_spectrum,lambdas_dist) \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell above, we left the results with the workers, rather than copying the results back to the delegator process.\n",
    "In the cell below, we will call `collect` the data in order to copy all the data back to to the delegator process.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T02:19:09.082000-05:00",
     "start_time": "2019-02-25T07:17:40.167Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Timing calls to collect(map(...)).\n",
      " 12.604053 seconds (7.36 M allocations: 264.354 MiB, 0.61% gc time, 1.30% compilation time)\n",
      " 12.354655 seconds (7.11 M allocations: 247.591 MiB, 0.42% gc time)\n"
     ]
    }
   ],
   "source": [
    "println(\"# Timing calls to collect(map(...)).\")\n",
    "for i in 1:num_runs \n",
    "    @time collect(map(conv_spectrum,lambdas_dist)) \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copying all that data back for the delegator process to access added a significant amount to the total time.\n",
    "Sometimes you don't actually need to bring all the data back to the delegator process.  For example, you might have several calculations that can be done, each leaving the data distributed across many workers, until the very end.  We'll demonstrate that next.\n",
    "\n",
    "Imagine that we only wanted to compute the total flux within a filter bandpass (i.e., range of wavelengths).  Then we could write a function that determine each wavelength is inside the filter bandpass.  Then each worker could use that function to determine which points fall within the filter and return only the summed flux to reduce the amount of communications overhead.\n",
    "\n",
    "First, we'll want to tell each worker process what range of wavelengths to sum over.  To simplify the syntax, I'm using the [ParallelDataTransfer.jl](https://github.com/ChrisRackauckas/ParallelDataTransfer.jl) package's `sendto` function to explictly send data from the delegator to each of the worker processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sendto(workers(), lambda_min=lambda_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sendto(workers(), lambda_max=lambda_min+100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that different processors can have different values stored in the same variable name.  For example above, we send `lambda_max` on the worker process to a different value than on the delegator process, as verified below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7500.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4600.0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@getfrom first(workers()) lambda_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes that can be helpful (i.e., each worker can process data using its own variables without interfereing with other processes).  But sometimes that can be create confusion and bugs.  It's very easy to write bugs when using unconstrained parallel programming.  \n",
    "\n",
    "In order to ease the development, debugging and maintance of parallel code, it's best to use common patterns when possible. \n",
    "For example, one common scenario is that you want to performing a task that can be phrased as a [`mapreduce`](https://en.wikipedia.org/wiki/MapReduce) programming pattern.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function on each of the workers\n",
    "@everywhere is_in_filter_band(x) = (lambda_min::Float64 < x < lambda_max::Float64) ? one(x) : zero(x)\n",
    "\n",
    "# Run mapreduce, summing the product of the convolved spectrum and the filter's weight at each wavelength\n",
    "mapreduce(x->is_in_filter_band(x)*conv_spectrum(x), +, lambdas_dist);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T02:19:32.528000-05:00",
     "start_time": "2019-02-25T07:17:40.172Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Timing calls to mapreduce.\n",
      "  4.360586 seconds (99.76 k allocations: 6.593 MiB, 0.42% gc time, 2.17% compilation time)\n",
      "  4.218630 seconds (580 allocations: 28.141 KiB)\n"
     ]
    }
   ],
   "source": [
    "println(\"# Timing calls to mapreduce.\")\n",
    "for i in 1:num_runs \n",
    "    @time mapreduce(x->is_in_filter_band(x)*conv_spectrum(x), +, lambdas_dist) \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write output (without interfering with output from other jobs)\n",
    "In principle, you could have each process read and write from disk.  If carefully orchestrated this could improve disk performance.  However, if done casually it can easily result in worse perofrmance.  Therefore, we'll simplify things by having the delegator process handle all the disk input/output.  That way two processes won't be trying to write to the disk at the same time.\n",
    "Additionally, we'll use the SLURM_JOB_ID environment variable to make sure that different jobs don't overwrite each other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-25T02:19:49.937000-05:00",
     "start_time": "2019-02-25T07:17:40.174Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"ex2_out_6079270.csv\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = collect(map(conv_spectrum,lambdas_dist)) \n",
    "\n",
    "output_filename = \"ex2_out.csv\"\n",
    "if haskey(ENV,\"SLURM_JOB_ID\")         # see if PBS_JOBID is among environment variables\n",
    "   jobid_num = ENV[\"SLURM_JOB_ID\"]\n",
    "   output_filename = \"ex2_out_\" * jobid_num * \".csv\"\n",
    "end\n",
    "CSV.write(output_filename,DataFrame(lambda=lambdas,result=results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking performance versus number of workers\n",
    "To see how the performance scales on the number of workers, we'll gradually remove worker processes and compare the performance of one of our functions to compute the mean squared error between two spectra from Lab 6, exercise 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@everywhere function calc_mse_mapreduce(lambdas::AbstractArray, spec1::AbstractSpectrum, spec2::AbstractSpectrum,  v::Number)\n",
    "    c = lab7.speed_of_light\n",
    "    z = v/c \n",
    "    spec2_shifted = doppler_shifted_spectrum(spec2,z)\n",
    "    mse = mapreduce(x->(spec1(x)-spec2_shifted(x))^2, +, lambdas)\n",
    "    mse /= length(lambdas)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  8.910928 seconds (261.90 k allocations: 17.555 MiB, 1.91% compilation time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.019157581014916e-10"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Force function to compile\n",
    "@time calc_mse_mapreduce(lambdas_dist, conv_spectrum, conv_spectrum, 10.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Timing calls to mapreduce on DArray with 4 workers.\n",
      "8.281837428 seconds.\n",
      "# Timing calls to mapreduce on DArray with 3 workers.\n",
      "10.850171809 seconds.\n",
      "# Timing calls to mapreduce on DArray with 2 workers.\n",
      "16.200637826 seconds.\n",
      "# Timing calls to mapreduce on DArray with 1 workers.\n"
     ]
    }
   ],
   "source": [
    "num_workers_all = nworkers()\n",
    "wall_time = zeros(num_workers_all)\n",
    "for nw in num_workers_all:-1:1\n",
    "    println(\"# Timing calls to mapreduce on DArray with $nw workers.\")\n",
    "    wall_time[nw] = @elapsed calc_mse_mapreduce(lambdas_dist, conv_spectrum, conv_spectrum, 10.0) \n",
    "    println(wall_time[nw], \" seconds.\")\n",
    "    if nw > 1\n",
    "        rmprocs(last(workers()))            # Remove one worker\n",
    "        global lambdas_dist = distribute(lambdas)  # Redistribute wavelengths over remaining workers\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(At this point we've removed the worker processes.  If you want to run things in parallel, you'll need to restart the notebook.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"# Timing calls to mapreduce (serial).\")\n",
    "@time calc_mse_mapreduce(lambdas, conv_spectrum, conv_spectrum, 10.0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.2",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
